{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('msd_full.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  53.39967,   56.67781,   55.63508, ...,    1.3159 ,    2.37448,\n",
       "           1.85999],\n",
       "       [  42.83464,  -33.90478,   38.97704, ...,    9.72827, -175.51446,\n",
       "         -15.8389 ],\n",
       "       [  49.88591,   23.30563,   54.79012, ...,    1.94152,  101.27241,\n",
       "           8.99355],\n",
       "       ...,\n",
       "       [  48.69997,   72.78532,   27.79159, ...,   -1.70515,   39.9382 ,\n",
       "           4.83517],\n",
       "       [  49.45505,   56.51035,   15.80259, ...,  -10.0645 ,   42.57409,\n",
       "         -12.3541 ],\n",
       "       [  40.31555,  -29.88336,    8.87396, ...,    5.81771,  -50.36502,\n",
       "          -8.57407]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array(data['X_train'])\n",
    "y_train = np.array(data['Y_train'])\n",
    "X_test = np.array(data['X_test'])\n",
    "y_test = np.array(data['Y_test'])\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_subtrain, X_validation, y_subtrain, y_validation = train_test_split(X_train, y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 (90%):\n",
    "Train and tune the models listed above. Report test RMSE for each model setting.   \n",
    "# OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.64730436,  1.07368824,  1.33234846, ..., -0.23489897,\n",
       "        -0.0939435 ,  0.02434362],\n",
       "       [-0.09061289, -0.68133197,  0.8599771 , ...,  0.39661381,\n",
       "        -1.05632047, -0.77849533],\n",
       "       [ 1.06930088,  0.42710869,  1.30838796, ..., -0.18793396,\n",
       "         0.44109309,  0.34792887],\n",
       "       ...,\n",
       "       [ 0.74586166,  1.17827461,  0.69388217, ..., -0.53936303,\n",
       "        -0.6605043 , -0.3324114 ],\n",
       "       [ 0.64885934, -0.24098716,  0.05695651, ..., -1.08791946,\n",
       "        -1.07260746, -3.00830262],\n",
       "       [ 1.13937502,  0.07639194,  0.39806435, ..., -0.27280759,\n",
       "         0.09099401, -0.15508672]])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.560350209959099"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression().fit(X_subtrain[0:10000], y_subtrain[0:10000])\n",
    "y_pred = reg.predict(X_test)\n",
    "rmse = np.sqrt(((y_pred - y_test) ** 2).mean())\n",
    "\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP_0_dm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading package and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2l\n",
    "from mxnet import autograd, gluon, np, npx\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=False):\n",
    "    \"\"\"Construct a Gluon data loader\"\"\"\n",
    "    dataset = gluon.data.ArrayDataset(*data_arrays)\n",
    "    return gluon.data.DataLoader(dataset, batch_size, shuffle=is_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_subtrain[0:10000].astype('float32')\n",
    "labels = (y_subtrain[0:10000] - y_train.mean()).astype('float32')\n",
    "\n",
    "batch_size = 10000\n",
    "data_iter = load_array((features, labels), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon import loss as gloss\n",
    "from mxnet import init\n",
    "from mxnet import gluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3000, loss: 77.469025\n",
      " , rate =  0.1\n",
      "rmse:  9.594734078489857\n",
      "epoch 3000, loss: 76.727158\n",
      " , rate =  0.05\n",
      "rmse:  9.594735320798623\n",
      "epoch 3000, loss: 76.441711\n",
      " , rate =  0.01\n",
      "rmse:  9.59521700024044\n"
     ]
    }
   ],
   "source": [
    "rates = [0.1, 0.05, 0.01]\n",
    "for rate in rates:\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    loss = gloss.L2Loss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': rate})\n",
    "\n",
    "    num_epochs = 3000\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for X, y in data_iter:\n",
    "            with autograd.record():\n",
    "                l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        l = loss(net(np.array(features)), y)\n",
    "    \n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n",
    "    print(\" , rate = \", rate)\n",
    "    \n",
    "    pred = net(np.array(X_validation).astype('float32'))\n",
    "    print(\"rmse: \", np.sqrt(((pred.reshape(-1,) - np.array(y_validation - y_train.mean())) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We choose learing rate at 0.1 for our training.\n",
    "## Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3000, loss: 77.246964\n",
      " , rate =  0.01\n",
      "9.56082492348468\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(1))\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "loss = gloss.L2Loss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.01})\n",
    "\n",
    "num_epochs = 3000\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for X, y in data_iter:\n",
    "        with autograd.record():\n",
    "            l = loss(net(X), y)\n",
    "        l.backward()\n",
    "        trainer.step(batch_size)\n",
    "    l = loss(net(np.array(features)), y)\n",
    "    \n",
    "print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n",
    "print(\" , rate = \", rate)\n",
    "\n",
    "y_pred = net(np.array(X_test).astype('float32'))\n",
    "rmse = np.sqrt(((y_pred.reshape(-1,) - np.array(y_test - y_train.mean())) ** 2).mean())\n",
    "\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP_1_dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_subtrain[0:10000].astype('float32')\n",
    "labels = (y_subtrain[0:10000] - y_train.mean()).astype('float32')\n",
    "\n",
    "batch_size = 10000\n",
    "data_iter = load_array((features, labels), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2999: loss 22.214\n",
      " , rate =  0.1\n",
      "rmse:  11.788026028807177\n",
      "Epoch 2999: loss 25.003\n",
      " , rate =  0.05\n",
      "rmse:  11.365048931062686\n",
      "Epoch 2999: loss 26.312\n",
      " , rate =  0.01\n",
      "rmse:  10.141809658083018\n",
      "Epoch 2999: loss 31.847\n",
      " , rate =  0.005\n",
      "rmse:  9.484435148499303\n"
     ]
    }
   ],
   "source": [
    "rates = [0.1, 0.05, 0.01, 0.005]\n",
    "\n",
    "for rate in rates:\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(45, activation='relu'),\n",
    "            nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': rate})\n",
    "\n",
    "    for epoch in range(3000):\n",
    "        train_loss= 0.\n",
    "        for X, y in data_iter:\n",
    "            # forward + backward\n",
    "            with autograd.record():\n",
    "                output = net(X)\n",
    "                L2loss = gloss.L2Loss()\n",
    "                loss = L2loss(output, y)\n",
    "            loss.backward()\n",
    "            # update parameters\n",
    "            trainer.step(batch_size)\n",
    "            # calculate training metrics\n",
    "            train_loss += loss.mean()\n",
    "    print(\"Epoch %d: loss %.3f\" % (epoch, train_loss))\n",
    "    print(\" , rate = \", rate)\n",
    "    \n",
    "    pred = net(np.array(X_validation).astype('float32'))\n",
    "    print(\"rmse: \", np.sqrt(((pred.reshape(-1,) - np.array(y_validation - y_train.mean())) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We choose learing rate at 0.005 for our training.\n",
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2999: loss 32.006\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(1))\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.005})\n",
    "\n",
    "for epoch in range(3000):\n",
    "    train_loss= 0.\n",
    "    for X, y in data_iter:\n",
    "        # forward + backward\n",
    "        with autograd.record():\n",
    "            output = net(X)\n",
    "            L2loss = gloss.L2Loss()\n",
    "            loss = L2loss(output, y)\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        trainer.step(batch_size)\n",
    "        # calculate training metrics\n",
    "        train_loss += loss.mean()\n",
    "print(\"Epoch %d: loss %.3f\" % (epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.51705727318843\n"
     ]
    }
   ],
   "source": [
    "y_pred = net(np.array(X_test).astype('float32'))\n",
    "rmse = np.sqrt(((y_pred.reshape(-1,) - np.array(y_test - y_train.mean())) ** 2).mean())\n",
    "\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP_2_dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_subtrain[0:10000].astype('float32')\n",
    "labels = (y_subtrain[0:10000] - y_train.mean()).astype('float32')\n",
    "\n",
    "batch_size = 10000\n",
    "data_iter = load_array((features, labels), batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2999: loss 8.068\n",
      " , rate =  0.1\n",
      "rmse:  12.771718529410819\n",
      "Epoch 2999: loss 8.314\n",
      " , rate =  0.05\n",
      "rmse:  11.519813252632463\n",
      "Epoch 2999: loss 16.177\n",
      " , rate =  0.01\n",
      "rmse:  11.689718268337575\n",
      "Epoch 2999: loss 18.413\n",
      " , rate =  0.005\n",
      "rmse:  10.112713427602086\n"
     ]
    }
   ],
   "source": [
    "rates = [0.1, 0.05, 0.01, 0.005]\n",
    "\n",
    "for rate in rates:\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(45, activation='relu'),\n",
    "            nn.Dense(45, activation='relu'),\n",
    "            nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': rate})\n",
    "\n",
    "    for epoch in range(3000):\n",
    "        train_loss= 0.\n",
    "        for X, y in data_iter:\n",
    "            # forward + backward\n",
    "            with autograd.record():\n",
    "                output = net(X)\n",
    "                L2loss = gloss.L2Loss()\n",
    "                loss = L2loss(output, y)\n",
    "            loss.backward()\n",
    "            # update parameters\n",
    "            trainer.step(batch_size)\n",
    "            # calculate training metrics\n",
    "            train_loss += loss.mean()\n",
    "    print(\"Epoch %d: loss %.3f\" % (epoch, train_loss))\n",
    "    print(\" , rate = \", rate)\n",
    "    \n",
    "    pred = net(np.array(X_validation).astype('float32'))\n",
    "    print(\"rmse: \", np.sqrt(((pred.reshape(-1,) - np.array(y_validation - y_train.mean())) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We choose learing rate at 0.005 for our training.\n",
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2999: loss 18.397\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(1))\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.005})\n",
    "\n",
    "for epoch in range(3000):\n",
    "    train_loss= 0.\n",
    "    for X, y in data_iter:\n",
    "        # forward + backward\n",
    "        with autograd.record():\n",
    "            output = net(X)\n",
    "            L2loss = gloss.L2Loss()\n",
    "            loss = L2loss(output, y)\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        trainer.step(batch_size)\n",
    "        # calculate training metrics\n",
    "        train_loss += loss.mean()\n",
    "print(\"Epoch %d: loss %.3f\" % (epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.780411714542982\n"
     ]
    }
   ],
   "source": [
    "y_pred = net(np.array(X_test).astype('float32'))\n",
    "rmse = np.sqrt(((y_pred.reshape(-1,) - np.array(y_test - y_train.mean())) ** 2).mean())\n",
    "\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP_2_dm_L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_subtrain[0:10000].astype('float32')\n",
    "labels = (y_subtrain[0:10000] - y_train.mean()).astype('float32')\n",
    "\n",
    "batch_size = 10000\n",
    "data_iter = load_array((features, labels), batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2999: loss 60.907\n",
      " , wd lambda =  5\n",
      "rmse:  10.929705132185605\n",
      "Epoch 2999: loss 60.906\n",
      " , wd lambda =  1\n",
      "rmse:  10.929751922515324\n",
      "Epoch 2999: loss 60.906\n",
      " , wd lambda =  0.5\n",
      "rmse:  10.929792581386222\n",
      "Epoch 2999: loss 9.654\n",
      " , wd lambda =  0\n",
      "rmse:  12.203502508993948\n"
     ]
    }
   ],
   "source": [
    "wds = [5, 1, 0.5, 0]\n",
    "\n",
    "for wd in wds:\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(45, activation='relu'),\n",
    "            nn.Dense(45, activation='relu'),\n",
    "            nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.005, 'wd': wd})\n",
    "\n",
    "    for epoch in range(3000):\n",
    "        train_loss= 0.\n",
    "        for X, y in data_iter:\n",
    "            # forward + backward\n",
    "            with autograd.record():\n",
    "                output = net(X)\n",
    "                L2loss = gloss.L2Loss()\n",
    "                loss = L2loss(output, y)\n",
    "            loss.backward()\n",
    "            # update parameters\n",
    "            trainer.step(batch_size)\n",
    "            # calculate training metrics\n",
    "            train_loss += loss.mean()\n",
    "    print(\"Epoch %d: loss %.3f\" % (epoch, train_loss))\n",
    "    print(\" , wd lambda = \", wd)\n",
    "    \n",
    "    pred = net(np.array(X_validation).astype('float32'))\n",
    "    print(\"rmse: \", np.sqrt(((pred.reshape(-1,) - np.array(y_validation - y_train.mean())) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We choose weight-decay lambda at 5 for our training.\n",
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2999: loss 60.906\n"
     ]
    }
   ],
   "source": [
    "wd = 5\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(1))\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.005, 'wd': wd})\n",
    "net.collect_params('.*bias').setattr('wd_mult', 0)\n",
    "\n",
    "for epoch in range(3000):\n",
    "    train_loss= 0.\n",
    "    for X, y in data_iter:\n",
    "        # forward + backward\n",
    "        with autograd.record():\n",
    "            output = net(X)\n",
    "            L2loss = gloss.L2Loss()\n",
    "            loss = L2loss(output, y)\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        trainer.step(batch_size)\n",
    "        # calculate training metrics\n",
    "        train_loss += loss.mean()\n",
    "print(\"Epoch %d: loss %.3f\" % (epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.853352511451803\n"
     ]
    }
   ],
   "source": [
    "y_pred = net(np.array(X_test).astype('float32'))\n",
    "rmse = np.sqrt(((y_pred.reshape(-1,) - np.array(y_test - y_train.mean())) ** 2).mean())\n",
    "\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP_2_dm_dropout\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_subtrain[0:10000].astype('float32')\n",
    "labels = (y_subtrain[0:10000] - y_train.mean()).astype('float32')\n",
    "\n",
    "batch_size = 10000\n",
    "data_iter = load_array((features, labels), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2999: loss 28.869\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(45, activation='relu'),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Dense(45, activation='relu'),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Dense(1))\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.005})\n",
    "net.collect_params('.*bias').setattr('wd_mult', 0)\n",
    "\n",
    "for epoch in range(3000):\n",
    "    train_loss= 0.\n",
    "    for X, y in data_iter:\n",
    "        # forward + backward\n",
    "        with autograd.record():\n",
    "            output = net(X)\n",
    "            L2loss = gloss.L2Loss()\n",
    "            loss = L2loss(output, y)\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        trainer.step(batch_size)\n",
    "        # calculate training metrics\n",
    "        train_loss += loss.mean()\n",
    "print(\"Epoch %d: loss %.3f\" % (epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.694782538082917\n"
     ]
    }
   ],
   "source": [
    "y_pred = net(np.array(X_test).astype('float32'))\n",
    "rmse = np.sqrt(((y_pred.reshape(-1,) - np.array(y_test - y_train.mean())) ** 2).mean())\n",
    "\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP_2_ykeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_subtrain[0:10000].astype('float32')\n",
    "labels = (y_subtrain[0:10000]).astype('float32')\n",
    "\n",
    "batch_size = 10000\n",
    "data_iter = load_array((features, labels), batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2999: loss 60.906\n",
      " , rate =  1\n",
      "rmse:  10.929908131989963\n",
      "Epoch 2999: loss 60.906\n",
      " , rate =  0.5\n",
      "rmse:  10.929908887816316\n",
      "Epoch 2999: loss 60.906\n",
      " , rate =  0.1\n",
      "rmse:  10.929911924754585\n",
      "Epoch 2999: loss 60.906\n",
      " , rate =  0.05\n",
      "rmse:  10.929915751601321\n"
     ]
    }
   ],
   "source": [
    "rates = [1, 0.5, 0.1, 0.05]\n",
    "\n",
    "for rate in rates:\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(45, activation='relu'),\n",
    "            nn.Dense(45, activation='relu'),\n",
    "            nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.1))\n",
    "\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': rate})\n",
    "\n",
    "    for epoch in range(3000):\n",
    "        train_loss= 0.\n",
    "        for X, y in data_iter:\n",
    "            # forward + backward\n",
    "            with autograd.record():\n",
    "                output = net(X)\n",
    "                L2loss = gloss.L2Loss()\n",
    "                loss = L2loss(output, y)\n",
    "            loss.backward()\n",
    "            # update parameters\n",
    "            trainer.step(batch_size)\n",
    "            # calculate training metrics\n",
    "            train_loss += loss.mean()\n",
    "    print(\"Epoch %d: loss %.3f\" % (epoch, train_loss))\n",
    "    print(\" , rate = \", rate)\n",
    "    \n",
    "    pred = net(np.array(X_validation).astype('float32'))\n",
    "    print(\"rmse: \", np.sqrt(((pred.reshape(-1,) - np.array(y_validation)) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We choose rate at 1 for our training.\n",
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2999: loss 60.906\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(1))\n",
    "net.initialize(init.Normal(sigma=0.1))\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 1})\n",
    "\n",
    "for epoch in range(3000):\n",
    "    train_loss= 0.\n",
    "    for X, y in data_iter:\n",
    "        # forward + backward\n",
    "        with autograd.record():\n",
    "            output = net(X)\n",
    "            L2loss = gloss.L2Loss()\n",
    "            loss = L2loss(output, y)\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        trainer.step(batch_size)\n",
    "        # calculate training metrics\n",
    "        train_loss += loss.mean()\n",
    "print(\"Epoch %d: loss %.3f\" % (epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.853351746156997\n"
     ]
    }
   ],
   "source": [
    "y_pred = net(np.array(X_test).astype('float32'))\n",
    "rmse = np.sqrt(((y_pred.reshape(-1,) - np.array(y_test)) ** 2).mean())\n",
    "\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP_2_ykeep_L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_subtrain[0:10000].astype('float32')\n",
    "labels = (y_subtrain[0:10000]).astype('float32')\n",
    "\n",
    "batch_size = 10000\n",
    "data_iter = load_array((features, labels), batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2999: loss 499220.750\n",
      " , wd lambda =  1\n",
      "rmse:  999.286933469116\n",
      "Epoch 2999: loss 221909.938\n",
      " , wd lambda =  0.5\n",
      "rmse:  666.2639573799345\n",
      "Epoch 2999: loss 60.906\n",
      " , wd lambda =  0\n",
      "rmse:  10.929911924754585\n"
     ]
    }
   ],
   "source": [
    "wds = [1, 0.5, 0]\n",
    "\n",
    "for wd in wds:\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(45, activation='relu'),\n",
    "            nn.Dense(45, activation='relu'),\n",
    "            nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.1))\n",
    "\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1, 'wd': wd})\n",
    "\n",
    "    for epoch in range(3000):\n",
    "        train_loss= 0.\n",
    "        for X, y in data_iter:\n",
    "            # forward + backward\n",
    "            with autograd.record():\n",
    "                output = net(X)\n",
    "                L2loss = gloss.L2Loss()\n",
    "                loss = L2loss(output, y)\n",
    "            loss.backward()\n",
    "            # update parameters\n",
    "            trainer.step(batch_size)\n",
    "            # calculate training metrics\n",
    "            train_loss += loss.mean()\n",
    "    print(\"Epoch %d: loss %.3f\" % (epoch, train_loss))\n",
    "    print(\" , wd lambda = \", wd)\n",
    "    \n",
    "    pred = net(np.array(X_validation).astype('float32'))\n",
    "    print(\"rmse: \", np.sqrt(((pred.reshape(-1,) - np.array(y_validation)) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2999: loss 256.646\n",
      " , wd lambda =  0.01\n",
      "rmse:  22.663183547949902\n",
      "Epoch 2999: loss 110.332\n",
      " , wd lambda =  0.005\n",
      "rmse:  14.820958140469005\n",
      "Epoch 2999: loss 62.900\n",
      " , wd lambda =  0.001\n",
      "rmse:  11.122982395885943\n"
     ]
    }
   ],
   "source": [
    "wds = [0.01, 0.005, 0.001]\n",
    "\n",
    "for wd in wds:\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(45, activation='relu'),\n",
    "            nn.Dense(45, activation='relu'),\n",
    "            nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.1))\n",
    "\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1, 'wd': wd})\n",
    "\n",
    "    for epoch in range(3000):\n",
    "        train_loss= 0.\n",
    "        for X, y in data_iter:\n",
    "            # forward + backward\n",
    "            with autograd.record():\n",
    "                output = net(X)\n",
    "                L2loss = gloss.L2Loss()\n",
    "                loss = L2loss(output, y)\n",
    "            loss.backward()\n",
    "            # update parameters\n",
    "            trainer.step(batch_size)\n",
    "            # calculate training metrics\n",
    "            train_loss += loss.mean()\n",
    "    print(\"Epoch %d: loss %.3f\" % (epoch, train_loss))\n",
    "    print(\" , wd lambda = \", wd)\n",
    "    \n",
    "    pred = net(np.array(X_validation).astype('float32'))\n",
    "    print(\"rmse: \", np.sqrt(((pred.reshape(-1,) - np.array(y_validation)) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We choose weight-decay lambda at 0.001 for our training.\n",
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2999: loss 60.906\n"
     ]
    }
   ],
   "source": [
    "wd = 0.001\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(1))\n",
    "net.initialize(init.Normal(sigma=0.1))\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1, 'wd': wd})\n",
    "net.collect_params('.*bias').setattr('wd_mult', 0)\n",
    "\n",
    "for epoch in range(3000):\n",
    "    train_loss= 0.\n",
    "    for X, y in data_iter:\n",
    "        # forward + backward\n",
    "        with autograd.record():\n",
    "            output = net(X)\n",
    "            L2loss = gloss.L2Loss()\n",
    "            loss = L2loss(output, y)\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        trainer.step(batch_size)\n",
    "        # calculate training metrics\n",
    "        train_loss += loss.mean()\n",
    "print(\"Epoch %d: loss %.3f\" % (epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.85336171210052\n"
     ]
    }
   ],
   "source": [
    "y_pred = net(np.array(X_test).astype('float32'))\n",
    "rmse = np.sqrt(((y_pred.reshape(-1,) - np.array(y_test)) ** 2).mean())\n",
    "\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP_2_ykeep_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_subtrain[0:10000].astype('float32')\n",
    "labels = (y_subtrain[0:10000]).astype('float32')\n",
    "\n",
    "batch_size = 10000\n",
    "data_iter = load_array((features, labels), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2999: loss 60.906\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(45, activation='relu'),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Dense(45, activation='relu'),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Dense(1))\n",
    "net.initialize(init.Normal(sigma=0.1))\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 1})\n",
    "net.collect_params('.*bias').setattr('wd_mult', 0)\n",
    "\n",
    "for epoch in range(3000):\n",
    "    train_loss= 0.\n",
    "    for X, y in data_iter:\n",
    "        # forward + backward\n",
    "        with autograd.record():\n",
    "            output = net(X)\n",
    "            L2loss = gloss.L2Loss()\n",
    "            loss = L2loss(output, y)\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        trainer.step(batch_size)\n",
    "        # calculate training metrics\n",
    "        train_loss += loss.mean()\n",
    "print(\"Epoch %d: loss %.3f\" % (epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.853351746156997\n"
     ]
    }
   ],
   "source": [
    "y_pred = net(np.array(X_test).astype('float32'))\n",
    "rmse = np.sqrt(((y_pred.reshape(-1,) - np.array(y_test)) ** 2).mean())\n",
    "\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP_2_dm_dropout_full\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_subtrain.astype('float32')\n",
    "labels = (y_subtrain - y_train.mean()).astype('float32')\n",
    "\n",
    "batch_size = len(X_subtrain)\n",
    "data_iter = load_array((features, labels), batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: loss 59.852\n",
      " , rate =  0.5\n",
      "rmse:  10.929698997234798\n",
      "Epoch 44: loss 59.840\n",
      " , rate =  0.1\n",
      "rmse:  10.928396746302868\n",
      "Epoch 44: loss 59.850\n",
      " , rate =  0.05\n",
      "rmse:  10.929526493338702\n"
     ]
    }
   ],
   "source": [
    "rates = [0.5, 0.1, 0.05]    \n",
    "for rate in rates:\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(45, activation='relu'),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Dense(45, activation='relu'),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': rate})\n",
    "    net.collect_params('.*bias').setattr('wd_mult', 0)\n",
    "\n",
    "    for epoch in range(45):\n",
    "        train_loss= 0.\n",
    "        for X, y in data_iter:\n",
    "            # forward + backward\n",
    "            with autograd.record():\n",
    "                output = net(X)\n",
    "                L2loss = gloss.L2Loss()\n",
    "                loss = L2loss(output, y)\n",
    "            loss.backward()\n",
    "            # update parameters\n",
    "            trainer.step(batch_size)\n",
    "            # calculate training metrics\n",
    "            train_loss += loss.mean()\n",
    "    print(\"Epoch %d: loss %.3f\" % (epoch, train_loss))\n",
    "    print(\" , rate = \", rate)\n",
    "    \n",
    "    pred = net(np.array(X_validation).astype('float32'))\n",
    "    print(\"rmse: \", np.sqrt(((pred.reshape(-1,) - np.array(y_validation - y_train.mean())) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: loss 58.475\n",
      " , sigma =  0.05\n",
      "rmse:  10.447137595888048\n",
      "Epoch 44: loss 59.831\n",
      " , sigma =  0.01\n",
      "rmse:  10.927359071768528\n",
      "Epoch 44: loss 59.851\n",
      " , sigma =  0.005\n",
      "rmse:  10.929675632377949\n"
     ]
    }
   ],
   "source": [
    "sigmas = [0.05, 0.01, 0.005]    \n",
    "for sigma in sigmas:\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(45, activation='relu'),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Dense(45, activation='relu'),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=sigma))\n",
    "\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})\n",
    "    net.collect_params('.*bias').setattr('wd_mult', 0)\n",
    "\n",
    "    for epoch in range(45):\n",
    "        train_loss= 0.\n",
    "        for X, y in data_iter:\n",
    "            # forward + backward\n",
    "            with autograd.record():\n",
    "                output = net(X)\n",
    "                L2loss = gloss.L2Loss()\n",
    "                loss = L2loss(output, y)\n",
    "            loss.backward()\n",
    "            # update parameters\n",
    "            trainer.step(batch_size)\n",
    "            # calculate training metrics\n",
    "            train_loss += loss.mean()\n",
    "    print(\"Epoch %d: loss %.3f\" % (epoch, train_loss))\n",
    "    print(\" , sigma = \", sigma)\n",
    "    \n",
    "    pred = net(np.array(X_validation).astype('float32'))\n",
    "    print(\"rmse: \", np.sqrt(((pred.reshape(-1,) - np.array(y_validation - y_train.mean())) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We choose learning rate at 0.1 for training and normal distribution sigma = 0.05 for initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: loss 51.525\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(45, activation='relu'),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Dense(45, activation='relu'),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Dense(1))\n",
    "net.initialize(init.Normal(sigma=0.05))\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})\n",
    "net.collect_params('.*bias').setattr('wd_mult', 0)\n",
    "\n",
    "for epoch in range(45):\n",
    "    train_loss= 0.\n",
    "    for X, y in data_iter:\n",
    "        # forward + backward\n",
    "        with autograd.record():\n",
    "            output = net(X)\n",
    "            L2loss = gloss.L2Loss()\n",
    "            loss = L2loss(output, y)\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        trainer.step(batch_size)\n",
    "        # calculate training metrics\n",
    "        train_loss += loss.mean()\n",
    "print(\"Epoch %d: loss %.3f\" % (epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.688246102835802\n"
     ]
    }
   ],
   "source": [
    "y_pred = net(np.array(X_test).astype('float32'))\n",
    "rmse = np.sqrt(((y_pred.reshape(-1,) - np.array(y_test - y_train.mean())) ** 2).mean())\n",
    "\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 (15%):\n",
    "Summarize test RMSE in one table. Discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OLS</td>\n",
       "      <td>9.56035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP_0_dm</td>\n",
       "      <td>9.56082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP_1_dm</td>\n",
       "      <td>9.51706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP_2_dm</td>\n",
       "      <td>10.7804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLP_2_dm_L2</td>\n",
       "      <td>10.8534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP_2_dm_dropout</td>\n",
       "      <td>9.69478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLP_2_ykeep</td>\n",
       "      <td>10.8534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLP_2_ykeep_L2</td>\n",
       "      <td>10.8534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MLP_2_ykeep_dropout</td>\n",
       "      <td>10.8534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLP_2_dm_dropout_full</td>\n",
       "      <td>9.68825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Case     RMSE\n",
       "0                    OLS  9.56035\n",
       "1               MLP_0_dm  9.56082\n",
       "2               MLP_1_dm  9.51706\n",
       "3               MLP_2_dm  10.7804\n",
       "4            MLP_2_dm_L2  10.8534\n",
       "5       MLP_2_dm_dropout  9.69478\n",
       "6            MLP_2_ykeep  10.8534\n",
       "7         MLP_2_ykeep_L2  10.8534\n",
       "8    MLP_2_ykeep_dropout  10.8534\n",
       "9  MLP_2_dm_dropout_full  9.68825"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rmses = [9.560350209959099,  9.56082492348468, 9.51705727318843, 10.780411714542982, 10.853352511451803,\n",
    "         9.694782538082917, 10.853351746156997, 10.85336171210052, 10.853351746156997, 9.688246102835802]\n",
    "\n",
    "cases = [\"OLS\", \"MLP_0_dm\", \"MLP_1_dm\", \"MLP_2_dm\", \"MLP_2_dm_L2\", \"MLP_2_dm_dropout\", \n",
    "        \"MLP_2_ykeep\", \"MLP_2_ykeep_L2\", \"MLP_2_ykeep_dropout\", \"MLP_2_dm_dropout_full\"]\n",
    "\n",
    "table = pd.DataFrame([cases, rmses]).T\n",
    "table.columns = [\"Case\", \"RMSE\"]\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 效果最好的為單層的MLP，OLS次之\n",
    "* 對Y進行的De-mean效果好於未De-mean的版本\n",
    "* 在有De-mean的情況下：\n",
    "  + 使用Weight-decay會降低效果，可能因為Model不夠powerful\n",
    "  + 使用Dropout則顯著提升效果\n",
    "* 未De-mean的話則效果不顯著\n",
    "* 在De-mean及Drop-out的情況下，使用全部的資料效果好於只使用前10000筆"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
