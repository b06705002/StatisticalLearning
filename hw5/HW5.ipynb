{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Q1</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Data</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import autograd, gluon, np, npx\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import time\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('msd_full.pickle', 'rb') as f:\n",
    "     data = pickle.load(f)     \n",
    "# print(np.mean(data['X_train'].astype('float32')).shape)\n",
    "# print(X_train.mean(axis=0))\n",
    "# scaler = preprocessing.StandardScaler().fit(data['X_train'].astype('float32'))\n",
    "\n",
    "X_train = data['X_train'].astype('float32')\n",
    "X_test = data['X_test'].astype('float32')\n",
    "Y_train = data['Y_train'].astype('float32')\n",
    "Y_test = data['Y_test'].astype('float32')\n",
    "# print(X_train)\n",
    "# print(X_test)\n",
    "# print(Y_train)\n",
    "# print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_subtrain, X_val, Y_subtrain, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=0)\n",
    "# print(len(X_subtrain))\n",
    "# print(len(X_val))\n",
    "# print(len(Y_subtrain))\n",
    "# print(len(Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>OLS</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.553146\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "reg = LinearRegression().fit(X_subtrain[:10000], Y_subtrain[:10000])\n",
    "py = reg.predict(X_test)\n",
    "# print(time.time()-s)\n",
    "rmse = np.sqrt(((py - Y_test) ** 2).mean())\n",
    "print(rmse)\n",
    "# print(rmse, time.time()-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE: 9.553146"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>MLP_0_dm</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True):  #@save\n",
    "    \"\"\"Construct a Gluon data loader.\"\"\"\n",
    "    dataset = gluon.data.ArrayDataset(*data_arrays)\n",
    "    return gluon.data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "Delabels = Y_subtrain[:10000] - Y_train.mean()\n",
    "# DeY_val = np.array(Y_val) - np.mean(np.array(Y_train))\n",
    "# DeY_test = np.array(Y_test) - np.mean(np.array(Y_train))\n",
    "\n",
    "features = X_subtrain[:10000]\n",
    "                        \n",
    "batch_size = 10000\n",
    "# features = np.array(X_subtrain)[:10000]\n",
    "# labels = np.array(Y_subtrain)[:10000]\n",
    "data_iter = load_array((features, Delabels), batch_size)\n",
    "# next(iter(data_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202.5854229927063\n",
      "learning rate:\n",
      "0.005\n",
      "epoch 3000, loss: 72.680191\n",
      "RMSE: \n",
      "9.575216\n",
      "208.5375485420227\n",
      "learning rate:\n",
      "0.01\n",
      "epoch 3000, loss: 72.269600\n",
      "RMSE: \n",
      "9.573734\n",
      "212.8100893497467\n",
      "learning rate:\n",
      "0.03\n",
      "epoch 3000, loss: 72.040260\n",
      "RMSE: \n",
      "9.575488\n"
     ]
    }
   ],
   "source": [
    "from mxnet.gluon import nn\n",
    "from mxnet import init\n",
    "from mxnet.gluon import loss as gloss\n",
    "from mxnet import gluon\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 3000\n",
    "# print(X_subtrain.type())\n",
    "# print(labels.shape)\n",
    "\n",
    "l_list = [0.005,0.01, 0.03]\n",
    "for z in l_list:\n",
    "    s = time.time()\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    loss = gloss.L2Loss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': z})\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for X, y in data_iter:\n",
    "            with autograd.record():\n",
    "                l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        l = loss(net(np.array(features)), np.array(y))\n",
    "    print(time.time() -s)        \n",
    "    print('learning rate:')\n",
    "    print(z)\n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n",
    "    \n",
    "    y_pred = net(np.array(X_val))\n",
    "    print(\"RMSE: \")\n",
    "    print(np.sqrt(((y_pred.reshape(-1,) - np.array(Y_val - Y_train.mean())) ** 2).mean()))\n",
    "    \n",
    "#     print(L)\n",
    "#     print('loss: %f' % L.mean().asnumpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best learning rate: 0.1\n",
    "<br>\n",
    "RMSE: 9.573734"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>With lr = 0.01, train the data</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224.4110758304596\n",
      "epoch 3000, loss: 72.111809\n",
      "RMSE: \n",
      "9.551596\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(1))\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "loss = gloss.L2Loss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.01})\n",
    "# for z in l_list:\n",
    "#     s = time.time()\n",
    "#     net = nn.Sequential()\n",
    "#     net.add(nn.Dense(1))\n",
    "#     net.initialize(init.Normal(sigma=0.01))\n",
    "#     loss = gloss.L2Loss()\n",
    "#     trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': z})\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for X, y in data_iter:\n",
    "        with autograd.record():\n",
    "            l = loss(net(X), y)\n",
    "        l.backward()\n",
    "        trainer.step(batch_size)\n",
    "    l = loss(net(np.array(features)), np.array(y))\n",
    "print(time.time() -s)        \n",
    "# print('learning rate:')\n",
    "# print(z)\n",
    "print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n",
    "    \n",
    "y_pred = net(np.array(X_test))\n",
    "print(\"RMSE: \")\n",
    "print(np.sqrt(((y_pred.reshape(-1,) - np.array(Y_test - Y_train.mean())) ** 2).mean()))\n",
    "    \n",
    "#     print(L)\n",
    "#     print('loss: %f' % L.mean().asnumpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>MLP_1_dm</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256.105345249176\n",
      "learning rate:\n",
      "0.005\n",
      "epoch 3000, loss: 82.555916\n",
      "RMSE: \n",
      "9.453445\n",
      "246.57781744003296\n",
      "learning rate:\n",
      "0.01\n",
      "epoch 3000, loss: 90.947136\n",
      "RMSE: \n",
      "10.128688\n",
      "246.3334720134735\n",
      "learning rate:\n",
      "0.03\n",
      "epoch 3000, loss: 92.257347\n",
      "RMSE: \n",
      "10.582736\n"
     ]
    }
   ],
   "source": [
    "data_iter = load_array((features, Delabels), batch_size)\n",
    "l_list = [0.005,0.01, 0.03]\n",
    "for z in l_list:\n",
    "    s = time.time()\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(45, activation='relu'),nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    loss = gloss.L2Loss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': z})\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for X, y in data_iter:\n",
    "            with autograd.record():\n",
    "                l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        l = loss(net(np.array(features)), np.array(y))\n",
    "    print(time.time() -s)        \n",
    "    print('learning rate:')\n",
    "    print(z)\n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n",
    "    \n",
    "    y_pred = net(np.array(X_val))\n",
    "    print(\"RMSE: \")\n",
    "    print(np.sqrt(((y_pred.reshape(-1,) - np.array(Y_val - Y_train.mean())) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248.78092646598816\n",
      "learning rate:\n",
      "0.001\n",
      "epoch 3000, loss: 74.934402\n",
      "RMSE: \n",
      "9.277115\n",
      "253.03954315185547\n",
      "learning rate:\n",
      "0.003\n",
      "epoch 3000, loss: 78.278465\n",
      "RMSE: \n",
      "9.227323\n"
     ]
    }
   ],
   "source": [
    "l_list = [0.001,0.003]\n",
    "for z in l_list:\n",
    "    s = time.time()\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(45, activation='relu'),nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    loss = gloss.L2Loss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': z})\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for X, y in data_iter:\n",
    "            with autograd.record():\n",
    "                l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        l = loss(net(np.array(features)), np.array(y))\n",
    "    print(time.time() -s)        \n",
    "    print('learning rate:')\n",
    "    print(z)\n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n",
    "    \n",
    "    y_pred = net(np.array(X_val))\n",
    "    print(\"RMSE: \")\n",
    "    print(np.sqrt(((y_pred.reshape(-1,) - np.array(Y_val - Y_train.mean())) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best learning rate: 0.003\n",
    "RMSE: 9.227323"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>With lr = 0.003, train the data</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246.5439064502716\n",
      "epoch 3000, loss: 79.750069\n",
      "RMSE: \n",
      "9.258854\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(45, activation='relu'),nn.Dense(1))\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "loss = gloss.L2Loss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.003})\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for X, y in data_iter:\n",
    "        with autograd.record():\n",
    "            l = loss(net(X), y)\n",
    "        l.backward()\n",
    "        trainer.step(batch_size)\n",
    "    l = loss(net(np.array(features)), np.array(y))\n",
    "print(time.time() -s)        \n",
    "# print('learning rate:')\n",
    "# print(z)\n",
    "print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n",
    "    \n",
    "y_pred = net(np.array(X_test))\n",
    "print(\"RMSE: \")\n",
    "print(np.sqrt(((y_pred.reshape(-1,) - np.array(Y_test - Y_train.mean())) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>MLP_2_dm</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292.5379502773285\n",
      "learning rate:\n",
      "0.005\n",
      "epoch 3000, loss: 88.397163\n",
      "RMSE: \n",
      "10.249074\n",
      "1591.5652911663055\n",
      "learning rate:\n",
      "0.01\n",
      "epoch 3000, loss: 89.039726\n",
      "RMSE: \n",
      "10.527457\n",
      "569.7644712924957\n",
      "learning rate:\n",
      "0.03\n",
      "epoch 3000, loss: 96.888977\n",
      "RMSE: \n",
      "10.937196\n"
     ]
    }
   ],
   "source": [
    "data_iter = load_array((features, Delabels), batch_size)\n",
    "l_list = [0.005,0.01, 0.03]\n",
    "for z in l_list:\n",
    "    s = time.time()\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    loss = gloss.L2Loss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': z})\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for X, y in data_iter:\n",
    "            with autograd.record():\n",
    "                l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        l = loss(net(np.array(features)), np.array(y))\n",
    "    print(time.time() -s)        \n",
    "    print('learning rate:')\n",
    "    print(z)\n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n",
    "    \n",
    "    y_pred = net(np.array(X_val))\n",
    "    print(\"RMSE: \")\n",
    "    print(np.sqrt(((y_pred.reshape(-1,) - np.array(Y_val - Y_train.mean())) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473.9112596511841\n",
      "learning rate:\n",
      "0.001\n",
      "epoch 3000, loss: 58.741264\n",
      "RMSE: \n",
      "10.904626\n",
      "552.5832614898682\n",
      "learning rate:\n",
      "0.003\n",
      "epoch 3000, loss: 80.048553\n",
      "RMSE: \n",
      "9.190431\n"
     ]
    }
   ],
   "source": [
    "l_list = [0.001, 0.003]\n",
    "for z in l_list:\n",
    "    s = time.time()\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    loss = gloss.L2Loss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': z})\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for X, y in data_iter:\n",
    "            with autograd.record():\n",
    "                l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        l = loss(net(np.array(features)), np.array(y))\n",
    "    print(time.time() -s)        \n",
    "    print('learning rate:')\n",
    "    print(z)\n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n",
    "    \n",
    "    y_pred = net(np.array(X_val))\n",
    "    print(\"RMSE: \")\n",
    "    print(np.sqrt(((y_pred.reshape(-1,) - np.array(Y_val - Y_train.mean())) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best learning rate: 0.003\n",
    "<br>\n",
    "RMSE: 9.190431"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>With lr = 0.003, train the data</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "585.7458832263947\n",
      "epoch 3000, loss: 78.621567\n",
      "RMSE: \n",
      "9.19598\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(1))\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "loss = gloss.L2Loss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.003})\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for X, y in data_iter:\n",
    "        with autograd.record():\n",
    "            l = loss(net(X), y)\n",
    "        l.backward()\n",
    "        trainer.step(batch_size)\n",
    "    l = loss(net(np.array(features)), np.array(y))\n",
    "print(time.time() -s)        \n",
    "print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n",
    "    \n",
    "y_pred = net(np.array(X_test))\n",
    "print(\"RMSE: \")\n",
    "print(np.sqrt(((y_pred.reshape(-1,) - np.array(Y_test - Y_train.mean())) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>MLP_2_dm_L2</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the tuning above, we can know that 0.003 is the best learning rate in MLP_2_dm.\n",
    "<br>\n",
    "So, we set lr as 0.003 and tune Weight Decay (L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364.05310583114624\n",
      "Weight Decay:\n",
      "0.01\n",
      "epoch 3000, loss: 80.136490\n",
      "RMSE: \n",
      "9.161269\n",
      "358.89979243278503\n",
      "Weight Decay:\n",
      "0.1\n",
      "epoch 3000, loss: 74.988632\n",
      "RMSE: \n",
      "9.20843\n",
      "391.8158349990845\n",
      "Weight Decay:\n",
      "1\n",
      "epoch 3000, loss: 58.741249\n",
      "RMSE: \n",
      "10.904879\n"
     ]
    }
   ],
   "source": [
    "from mxnet.gluon import nn\n",
    "from mxnet import init\n",
    "from mxnet.gluon import loss as gloss\n",
    "from mxnet import gluon\n",
    "num_epochs = 3000\n",
    "data_iter = load_array((features, Delabels), batch_size)\n",
    "wd_list = [0.01,0.1, 1]\n",
    "for wd in wd_list:\n",
    "    s = time.time()\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    loss = gloss.L2Loss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.003,\n",
    "                                                          'wd':wd})\n",
    "    net.collect_params('.*bias').setattr('wd_mult', 0)\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for X, y in data_iter:\n",
    "            with autograd.record():\n",
    "                l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        l = loss(net(np.array(features)), np.array(y))\n",
    "    print(time.time() -s)        \n",
    "    print('Weight Decay:')\n",
    "    print(wd)\n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n",
    "    \n",
    "    y_pred = net(np.array(X_val))\n",
    "    print(\"RMSE: \")\n",
    "    print(np.sqrt(((y_pred.reshape(-1,) - np.array(Y_val - Y_train.mean())) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366.1084289550781\n",
      "Weight Decay:\n",
      "0.005\n",
      "epoch 3000, loss: 80.321358\n",
      "RMSE: \n",
      "9.231958\n",
      "362.9658944606781\n",
      "Weight Decay:\n",
      "0.3\n",
      "epoch 3000, loss: 58.741249\n",
      "RMSE: \n",
      "10.904878\n"
     ]
    }
   ],
   "source": [
    "wd_list = [0.005,0.3]\n",
    "for wd in wd_list:\n",
    "    s = time.time()\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    loss = gloss.L2Loss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.003,\n",
    "                                                          'wd':wd})\n",
    "    net.collect_params('.*bias').setattr('wd_mult', 0)\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for X, y in data_iter:\n",
    "            with autograd.record():\n",
    "                l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        l = loss(net(np.array(features)), np.array(y))\n",
    "    print(time.time() -s)        \n",
    "    print('Weight Decay:')\n",
    "    print(wd)\n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n",
    "    \n",
    "    y_pred = net(np.array(X_val))\n",
    "    print(\"RMSE: \")\n",
    "    print(np.sqrt(((y_pred.reshape(-1,) - np.array(Y_val - Y_train.mean())) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366.43887734413147\n",
      "Weight Decay:\n",
      "0.03\n",
      "epoch 3000, loss: 80.307831\n",
      "RMSE: \n",
      "9.194692\n"
     ]
    }
   ],
   "source": [
    "wd_list = [0.03]\n",
    "for wd in wd_list:\n",
    "    s = time.time()\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    loss = gloss.L2Loss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.003,\n",
    "                                                          'wd':wd})\n",
    "    net.collect_params('.*bias').setattr('wd_mult', 0)\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for X, y in data_iter:\n",
    "            with autograd.record():\n",
    "                l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        l = loss(net(np.array(features)), np.array(y))\n",
    "    print(time.time() -s)        \n",
    "    print('Weight Decay:')\n",
    "    print(wd)\n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n",
    "    \n",
    "    y_pred = net(np.array(X_val))\n",
    "    print(\"RMSE: \")\n",
    "    print(np.sqrt(((y_pred.reshape(-1,) - np.array(Y_val - Y_train.mean())) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Weight Decay: 0.01\n",
    "<br>\n",
    "RMSE: 9.161269"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357.15456891059875\n",
      "epoch 3000, loss: 80.467972\n",
      "RMSE: \n",
      "9.178331\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(1))\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "loss = gloss.L2Loss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.003,\n",
    "                                                          'wd':0.01})\n",
    "net.collect_params('.*bias').setattr('wd_mult', 0)\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for X, y in data_iter:\n",
    "        with autograd.record():\n",
    "            l = loss(net(X), y)\n",
    "        l.backward()\n",
    "        trainer.step(batch_size)\n",
    "    l = loss(net(np.array(features)), np.array(y))\n",
    "print(time.time() -s)        \n",
    "print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n",
    "    \n",
    "y_pred = net(np.array(X_test))\n",
    "print(\"RMSE: \")\n",
    "print(np.sqrt(((y_pred.reshape(-1,) - np.array(Y_test - Y_train.mean())) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>MLP_2_dm_dropout</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442.92368721961975\n",
      "learning rate:\n",
      "0.005\n",
      "epoch 3000, loss: 76.755951\n",
      "RMSE: \n",
      "9.201779\n",
      "436.5309054851532\n",
      "learning rate:\n",
      "0.01\n",
      "epoch 3000, loss: 82.341736\n",
      "RMSE: \n",
      "9.497112\n",
      "434.0128080844879\n",
      "learning rate:\n",
      "0.03\n",
      "epoch 3000, loss: 85.361534\n",
      "RMSE: \n",
      "9.640176\n"
     ]
    }
   ],
   "source": [
    "data_iter = load_array((features, Delabels), batch_size)\n",
    "l_list = [0.005,0.01, 0.03]\n",
    "for z in l_list:\n",
    "    s = time.time()\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(45, activation='relu'),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Dense(45, activation='relu'),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    loss = gloss.L2Loss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': z})\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for X, y in data_iter:\n",
    "            with autograd.record():\n",
    "                l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        l = loss(net(np.array(features)), np.array(y))\n",
    "    print(time.time() -s)        \n",
    "    print('learning rate:')\n",
    "    print(z)\n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n",
    "    \n",
    "    y_pred = net(np.array(X_val))\n",
    "    print(\"RMSE: \")\n",
    "    print(np.sqrt(((y_pred.reshape(-1,) - np.array(Y_val - Y_train.mean())) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "434.21725630760193\n",
      "learning rate:\n",
      "0.003\n",
      "epoch 3000, loss: 73.444031\n",
      "RMSE: \n",
      "9.135549\n",
      "436.9496111869812\n",
      "learning rate:\n",
      "0.001\n",
      "epoch 3000, loss: 58.741108\n",
      "RMSE: \n",
      "10.904555\n"
     ]
    }
   ],
   "source": [
    "l_list = [0.003,0.001]\n",
    "for z in l_list:\n",
    "    s = time.time()\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(45, activation='relu'),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Dense(45, activation='relu'),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    loss = gloss.L2Loss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': z})\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for X, y in data_iter:\n",
    "            with autograd.record():\n",
    "                l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        l = loss(net(np.array(features)), np.array(y))\n",
    "    print(time.time() -s)        \n",
    "    print('learning rate:')\n",
    "    print(z)\n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n",
    "    \n",
    "    y_pred = net(np.array(X_val))\n",
    "    print(\"RMSE: \")\n",
    "    print(np.sqrt(((y_pred.reshape(-1,) - np.array(Y_val - Y_train.mean())) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best learning rate: 0.03\n",
    "<br>\n",
    "RMSE: 9.135549"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429.3790063858032\n",
      "epoch 3000, loss: 69.638359\n",
      "RMSE: \n",
      "9.17717\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(45, activation='relu'),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Dense(45, activation='relu'),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Dense(1))\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "loss = gloss.L2Loss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.003})\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for X, y in data_iter:\n",
    "        with autograd.record():\n",
    "            l = loss(net(X), y)\n",
    "        l.backward()\n",
    "        trainer.step(batch_size)\n",
    "    l = loss(net(np.array(features)), np.array(y))\n",
    "print(time.time() -s)        \n",
    "print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n",
    "    \n",
    "y_pred = net(np.array(X_test))\n",
    "print(\"RMSE: \")\n",
    "print(np.sqrt(((y_pred.reshape(-1,) - np.array(Y_test - Y_train.mean())) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>MLP_2_ykeep</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "from mxnet import init\n",
    "from mxnet.gluon import loss as gloss\n",
    "from mxnet import gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443.845591545105\n",
      "learning rate:\n",
      "0.005\n",
      "epoch 3000, loss: nan\n",
      "RMSE: \n",
      "nan\n",
      "405.42221117019653\n",
      "learning rate:\n",
      "0.01\n",
      "epoch 3000, loss: nan\n",
      "RMSE: \n",
      "nan\n",
      "402.7747790813446\n",
      "learning rate:\n",
      "0.03\n",
      "epoch 3000, loss: 58.741257\n",
      "RMSE: \n",
      "10.904878\n"
     ]
    }
   ],
   "source": [
    "labels = (Y_subtrain[0:10000]).astype('float32')\n",
    "num_epochs = 3000\n",
    "data_iter = load_array((features, labels), batch_size)\n",
    "l_list = [0.005,0.01, 0.03]\n",
    "for z in l_list:\n",
    "    s = time.time()\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    loss = gloss.L2Loss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': z})\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for X, y in data_iter:\n",
    "            with autograd.record():\n",
    "                l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        l = loss(net(np.array(features)), np.array(y))\n",
    "    print(time.time() -s)        \n",
    "    print('learning rate:')\n",
    "    print(z)\n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n",
    "    \n",
    "    y_pred = net(np.array(X_val))\n",
    "    print(\"RMSE: \")\n",
    "    print(np.sqrt(((y_pred.reshape(-1,) - np.array(Y_val)) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388.6475639343262\n",
      "learning rate:\n",
      "0.025\n",
      "epoch 3000, loss: nan\n",
      "RMSE: \n",
      "nan\n",
      "423.1871814727783\n",
      "learning rate:\n",
      "0.035\n",
      "epoch 3000, loss: 58.741249\n",
      "RMSE: \n",
      "10.904878\n"
     ]
    }
   ],
   "source": [
    "l_list = [0.025, 0.035]\n",
    "for z in l_list:\n",
    "    s = time.time()\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    loss = gloss.L2Loss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': z})\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for X, y in data_iter:\n",
    "            with autograd.record():\n",
    "                l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        l = loss(net(np.array(features)), np.array(y))\n",
    "    print(time.time() -s)        \n",
    "    print('learning rate:')\n",
    "    print(z)\n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n",
    "    \n",
    "    y_pred = net(np.array(X_val))\n",
    "    print(\"RMSE: \")\n",
    "    print(np.sqrt(((y_pred.reshape(-1,) - np.array(Y_val)) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best learning rate: 0.035\n",
    "<br>\n",
    "RMSE: 10.904878"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: \n",
      "10.851995\n"
     ]
    }
   ],
   "source": [
    "y_pred = net(np.array(X_test))\n",
    "print(\"RMSE: \")\n",
    "print(np.sqrt(((y_pred.reshape(-1,) - np.array(Y_test)) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>MLP_2_ykeep_L2</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the tuning above, we can know that 0.035 is the best learning rate in MLP_2_ykeep.\n",
    "<br>\n",
    "So, we set lr as 0.035 and tune Weight Decay (L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447.89610290527344\n",
      "Weight Decay:\n",
      "0.01\n",
      "epoch 3000, loss: 58.741249\n",
      "RMSE: \n",
      "10.904878\n",
      "456.81586503982544\n",
      "Weight Decay:\n",
      "0.1\n",
      "epoch 3000, loss: 58.741249\n",
      "RMSE: \n",
      "10.904878\n",
      "438.8902542591095\n",
      "Weight Decay:\n",
      "1\n",
      "epoch 3000, loss: nan\n",
      "RMSE: \n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "data_iter = load_array((features, labels), batch_size)\n",
    "wd_list = [0.01,0.1, 1]\n",
    "for wd in wd_list:\n",
    "    s = time.time()\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    loss = gloss.L2Loss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.035,\n",
    "                                                          'wd':wd})\n",
    "    net.collect_params('.*bias').setattr('wd_mult', 0)\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for X, y in data_iter:\n",
    "            with autograd.record():\n",
    "                l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        l = loss(net(np.array(features)), np.array(y))\n",
    "    print(time.time() -s)        \n",
    "    print('Weight Decay:')\n",
    "    print(wd)\n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n",
    "    \n",
    "    y_pred = net(np.array(X_val))\n",
    "    print(\"RMSE: \")\n",
    "    print(np.sqrt(((y_pred.reshape(-1,) - np.array(Y_val)) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407.0494329929352\n",
      "Weight Decay:\n",
      "0.001\n",
      "epoch 3000, loss: nan\n",
      "RMSE: \n",
      "nan\n",
      "419.7543668746948\n",
      "Weight Decay:\n",
      "0.005\n",
      "epoch 3000, loss: 58.741249\n",
      "RMSE: \n",
      "10.904878\n"
     ]
    }
   ],
   "source": [
    "wd_list = [0.001,0.005]\n",
    "for wd in wd_list:\n",
    "    s = time.time()\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    loss = gloss.L2Loss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.035,\n",
    "                                                          'wd':wd})\n",
    "    net.collect_params('.*bias').setattr('wd_mult', 0)\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for X, y in data_iter:\n",
    "            with autograd.record():\n",
    "                l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        l = loss(net(np.array(features)), np.array(y))\n",
    "    print(time.time() -s)        \n",
    "    print('Weight Decay:')\n",
    "    print(wd)\n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n",
    "    \n",
    "    y_pred = net(np.array(X_val))\n",
    "    print(\"RMSE: \")\n",
    "    print(np.sqrt(((y_pred.reshape(-1,) - np.array(Y_val)) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439.96631050109863\n",
      "Weight Decay:\n",
      "0.03\n",
      "epoch 3000, loss: nan\n",
      "RMSE: \n",
      "nan\n",
      "433.44333600997925\n",
      "Weight Decay:\n",
      "0.05\n",
      "epoch 3000, loss: nan\n",
      "RMSE: \n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "wd_list = [0.03,0.05]\n",
    "for wd in wd_list:\n",
    "    s = time.time()\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    loss = gloss.L2Loss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.035,\n",
    "                                                          'wd':wd})\n",
    "    net.collect_params('.*bias').setattr('wd_mult', 0)\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for X, y in data_iter:\n",
    "            with autograd.record():\n",
    "                l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        l = loss(net(np.array(features)), np.array(y))\n",
    "    print(time.time() -s)        \n",
    "    print('Weight Decay:')\n",
    "    print(wd)\n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n",
    "    \n",
    "    y_pred = net(np.array(X_val))\n",
    "    print(\"RMSE: \")\n",
    "    print(np.sqrt(((y_pred.reshape(-1,) - np.array(Y_val)) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Weight Decay: 0.005, 0.01\n",
    "<br>\n",
    "RMSE: 10.904878"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611.8387279510498\n",
      "Weight Decay:\n",
      "0.01\n",
      "epoch 3000, loss: 58.741249\n",
      "RMSE: \n",
      "10.851995\n"
     ]
    }
   ],
   "source": [
    "wd_list = [0.01]\n",
    "for wd in wd_list:\n",
    "    s = time.time()\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(45, activation='relu'),\n",
    "        nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    loss = gloss.L2Loss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.035,\n",
    "                                                          'wd':wd})\n",
    "    net.collect_params('.*bias').setattr('wd_mult', 0)\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for X, y in data_iter:\n",
    "            with autograd.record():\n",
    "                l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        l = loss(net(np.array(features)), np.array(y))\n",
    "    print(time.time() -s)        \n",
    "    print('Weight Decay:')\n",
    "    print(wd)\n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n",
    "    \n",
    "    y_pred = net(np.array(X_test))\n",
    "    print(\"RMSE: \")\n",
    "    print(np.sqrt(((y_pred.reshape(-1,) - np.array(Y_test)) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>MLP_2_ykeep_dropout</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575.1996710300446\n",
      "learning rate:\n",
      "0.005\n",
      "epoch 3000, loss: 58.741325\n",
      "RMSE: \n",
      "10.904878\n",
      "482.24423909187317\n",
      "learning rate:\n",
      "0.01\n",
      "epoch 3000, loss: 58.741268\n",
      "RMSE: \n",
      "10.904877\n",
      "78822.3553032875\n",
      "learning rate:\n",
      "0.03\n",
      "epoch 3000, loss: 58.741257\n",
      "RMSE: \n",
      "10.904878\n"
     ]
    }
   ],
   "source": [
    "data_iter = load_array((features, labels), batch_size)\n",
    "l_list = [0.005,0.01, 0.03]\n",
    "for z in l_list:\n",
    "    s = time.time()\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(45, activation='relu'),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Dense(45, activation='relu'),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    loss = gloss.L2Loss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': z})\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for X, y in data_iter:\n",
    "            with autograd.record():\n",
    "                l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        l = loss(net(np.array(features)), np.array(y))\n",
    "    print(time.time() -s)        \n",
    "    print('learning rate:')\n",
    "    print(z)\n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n",
    "    \n",
    "    y_pred = net(np.array(X_val))\n",
    "    print(\"RMSE: \")\n",
    "    print(np.sqrt(((y_pred.reshape(-1,) - np.array(Y_val)) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best learning rate: 0.01\n",
    "<br>\n",
    "RMSE: 10.904877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522.043089389801\n",
      "learning rate:\n",
      "0.01\n",
      "epoch 3000, loss: 58.741268\n",
      "RMSE: \n",
      "10.852015\n"
     ]
    }
   ],
   "source": [
    "l_list = [0.01]\n",
    "for z in l_list:\n",
    "    s = time.time()\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(45, activation='relu'),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Dense(45, activation='relu'),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    loss = gloss.L2Loss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': z})\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for X, y in data_iter:\n",
    "            with autograd.record():\n",
    "                l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        l = loss(net(np.array(features)), np.array(y))\n",
    "    print(time.time() -s)        \n",
    "    print('learning rate:')\n",
    "    print(z)\n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n",
    "    \n",
    "    y_pred = net(np.array(X_test))\n",
    "    print(\"RMSE: \")\n",
    "    print(np.sqrt(((y_pred.reshape(-1,) - np.array(Y_test)) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>MLP_2_dm_dropout_full</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Delabels = Y_subtrain-Y_train.mean()\n",
    "features = X_subtrain\n",
    "batch_size = len(Delabels)\n",
    "data_iter = load_array((features, Delabels), batch_size)\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328.31292843818665\n",
      "learning rate:\n",
      "0.005\n",
      "epoch 50, loss: 59.881229\n",
      "RMSE: \n",
      "10.905053\n",
      "305.0780680179596\n",
      "learning rate:\n",
      "0.01\n",
      "epoch 50, loss: 59.881226\n",
      "RMSE: \n",
      "10.90504\n",
      "290.1327557563782\n",
      "learning rate:\n",
      "0.03\n",
      "epoch 50, loss: 59.881222\n",
      "RMSE: \n",
      "10.905054\n"
     ]
    }
   ],
   "source": [
    "l_list = [0.005,0.01, 0.03]\n",
    "for z in l_list:\n",
    "    s = time.time()\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(45, activation='relu'),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Dense(45, activation='relu'),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    loss = gloss.L2Loss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': z})\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for X, y in data_iter:\n",
    "            with autograd.record():\n",
    "                l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        l = loss(net(np.array(features)), np.array(y))\n",
    "    print(time.time() -s)        \n",
    "    print('learning rate:')\n",
    "    print(z)\n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n",
    "    \n",
    "    y_pred = net(np.array(X_val))\n",
    "    print(\"RMSE: \")\n",
    "    print(np.sqrt(((y_pred.reshape(-1,) - np.array(Y_val - Y_train.mean())) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297.8623764514923\n",
      "learning rate:\n",
      "0.01\n",
      "epoch 50, loss: 59.881226\n",
      "RMSE: \n",
      "10.852482\n"
     ]
    }
   ],
   "source": [
    "l_list = [0.01]\n",
    "for z in l_list:\n",
    "    s = time.time()\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(45, activation='relu'),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Dense(45, activation='relu'),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    loss = gloss.L2Loss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': z})\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for X, y in data_iter:\n",
    "            with autograd.record():\n",
    "                l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        l = loss(net(np.array(features)), np.array(y))\n",
    "    print(time.time() -s)        \n",
    "    print('learning rate:')\n",
    "    print(z)\n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n",
    "    \n",
    "    y_pred = net(np.array(X_test))\n",
    "    print(\"RMSE: \")\n",
    "    print(np.sqrt(((y_pred.reshape(-1,) - np.array(Y_test - Y_train.mean())) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best learning rate: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Q2</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case    | # of obs. in subtraining| Model                   |Regularization    |y      | best RMSE         | learning rate|WD\n",
    "--------|:----------------:|-------------------------------:| ----------------:|------:|------------------:|-----:|-----:\n",
    "OLS     | 10000            |  Linear Regression             | None             |  keep | 9.553146          | NA   |NA\n",
    "MLP_0_dm| 10000            |  MLP, no hidden layers         | None             |de-mean| 9.551596          | 0.01 |NA\n",
    "MLP_1_dm| 10000            |  MLP, one hidden layer + ReLU  | None             |de-mean| 9.258854          | 0.003|NA\n",
    "MLP_2_dm| 10000            |  MLP, two hidden layers + ReLu | None             |de-mean| 9.19598           | 0.003|NA\n",
    "MLP_2_dm_L2| 10000         |  MLP, two hidden layers + ReLu | Weight Decay (L2)|de-mean| 9.161269          | 0.003|0.01\n",
    "MLP_2_dm_dropout| 10000    |  MLP, two hidden layers + ReLu | Dropout          |de-mean| 9.17717           | 0.03 |NA\n",
    "MLP_2_ykeep| 10000         |  MLP, two hidden layers + ReLu | None             |keep   | 10.851995         | 0.035|NA\n",
    "MLP_2_ykeep_L2| 10000      |  MLP, two hidden layers + ReLu | Weight Decay (L2)|keep   | 10.851995         | 0.035|0.01\n",
    "MLP_2_ykeep_dropout| 10000 |  MLP, two hidden layers + ReLu | Dropout          |keep   | 10.852015         | 0.01 |NA\n",
    "MLP_2_dm_dropout_full| all |  MLP, two hidden layers + ReLu | Dropout          |de-mean| 10.852482         | 0.01 |NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the table above, We can found that :\n",
    "<br>\n",
    "(1). The <strong>more layers</strong> the model uses, the <strong>better</strong> the model is(fewer RMSE).\n",
    "<br>\n",
    "(2). Using all data might not be better(maybe the difference of epoch number)\n",
    "<br>\n",
    "(3). y with de-mean performs <strong>much better</strong> than keep."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
